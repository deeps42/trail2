{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nprint(os.listdir(\"../input\"))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:57.412915Z","iopub.execute_input":"2023-04-12T00:45:57.413691Z","iopub.status.idle":"2023-04-12T00:45:57.420165Z","shell.execute_reply.started":"2023-04-12T00:45:57.413643Z","shell.execute_reply":"2023-04-12T00:45:57.418903Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:46:00.530581Z","iopub.execute_input":"2023-04-12T00:46:00.531095Z","iopub.status.idle":"2023-04-12T00:48:30.396104Z","shell.execute_reply.started":"2023-04-12T00:46:00.531054Z","shell.execute_reply":"2023-04-12T00:48:30.394210Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78befe7d61d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78befe7d66d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78befe7d68d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78befe7d6c10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78befe7cb110>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pyspark (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pyspark\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('ml-bank').getOrCreate()\ndf = spark.read.csv('../input/bank.csv', header = True, inferSchema = True)\ndf.printSchema()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-04-12T00:49:47.269289Z","iopub.execute_input":"2023-04-12T00:49:47.270263Z","iopub.status.idle":"2023-04-12T00:49:47.298319Z","shell.execute_reply.started":"2023-04-12T00:49:47.270208Z","shell.execute_reply":"2023-04-12T00:49:47.296978Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2100912931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ml-bank'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/bank.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"],"ename":"ModuleNotFoundError","evalue":"No module named 'pyspark'","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\npd.DataFrame(df.take(5), columns=df.columns).transpose()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.407286Z","iopub.status.idle":"2023-04-12T00:45:44.407790Z","shell.execute_reply.started":"2023-04-12T00:45:44.407537Z","shell.execute_reply":"2023-04-12T00:45:44.407588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analyzing Data**","metadata":{}},{"cell_type":"code","source":"df.groupby('deposit').count().toPandas()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.409738Z","iopub.status.idle":"2023-04-12T00:45:44.410224Z","shell.execute_reply.started":"2023-04-12T00:45:44.410014Z","shell.execute_reply":"2023-04-12T00:45:44.410037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\ndf.select(numeric_features).describe().toPandas().transpose()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.412790Z","iopub.status.idle":"2023-04-12T00:45:44.413438Z","shell.execute_reply.started":"2023-04-12T00:45:44.413104Z","shell.execute_reply":"2023-04-12T00:45:44.413138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_data = df.select(numeric_features).toPandas()\naxs = pd.scatter_matrix(numeric_data, figsize=(8, 8));\nn = len(numeric_data.columns)\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.415098Z","iopub.status.idle":"2023-04-12T00:45:44.415769Z","shell.execute_reply.started":"2023-04-12T00:45:44.415410Z","shell.execute_reply":"2023-04-12T00:45:44.415444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\ncols = df.columns\ndf.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.417492Z","iopub.status.idle":"2023-04-12T00:45:44.418124Z","shell.execute_reply.started":"2023-04-12T00:45:44.417822Z","shell.execute_reply":"2023-04-12T00:45:44.417857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n\ncategoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n\nstages = []\n\nfor categoricalCol in categoricalColumns:\n    \n    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n    \n    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    \n    stages += [stringIndexer, encoder]\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.420348Z","iopub.status.idle":"2023-04-12T00:45:44.420996Z","shell.execute_reply.started":"2023-04-12T00:45:44.420683Z","shell.execute_reply":"2023-04-12T00:45:44.420715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use the StringIndexer again to encode our labels to label indices. Next, we use the VectorAssembler to combine all the feature columns into a single vector column.","metadata":{}},{"cell_type":"code","source":"label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n\nstages += [label_stringIdx]\n\nnumericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n\nstages += [assembler]","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.423097Z","iopub.status.idle":"2023-04-12T00:45:44.423759Z","shell.execute_reply.started":"2023-04-12T00:45:44.423413Z","shell.execute_reply":"2023-04-12T00:45:44.423447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pipeline**\n\nWe use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow. A Pipeline’s stages are specified as an ordered array.","metadata":{}},{"cell_type":"code","source":"from pyspark.ml import Pipeline\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(df)\ndf = pipelineModel.transform(df)\nselectedCols = ['label', 'features'] + cols\ndf = df.select(selectedCols)\ndf.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.425174Z","iopub.status.idle":"2023-04-12T00:45:44.425822Z","shell.execute_reply.started":"2023-04-12T00:45:44.425481Z","shell.execute_reply":"2023-04-12T00:45:44.425515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(df.take(5), columns=df.columns).transpose()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.427264Z","iopub.status.idle":"2023-04-12T00:45:44.427701Z","shell.execute_reply.started":"2023-04-12T00:45:44.427479Z","shell.execute_reply":"2023-04-12T00:45:44.427501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = df.randomSplit([0.7, 0.3], seed = 2018)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.429735Z","iopub.status.idle":"2023-04-12T00:45:44.430379Z","shell.execute_reply.started":"2023-04-12T00:45:44.430044Z","shell.execute_reply":"2023-04-12T00:45:44.430078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating Logistic Regression Model**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\nlrModel = lr.fit(train)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.432245Z","iopub.status.idle":"2023-04-12T00:45:44.432905Z","shell.execute_reply.started":"2023-04-12T00:45:44.432576Z","shell.execute_reply":"2023-04-12T00:45:44.432611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta)\nplt.ylabel('Beta Coefficients')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.435781Z","iopub.status.idle":"2023-04-12T00:45:44.436440Z","shell.execute_reply.started":"2023-04-12T00:45:44.436100Z","shell.execute_reply":"2023-04-12T00:45:44.436134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainingSummary = lrModel.summary\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.438004Z","iopub.status.idle":"2023-04-12T00:45:44.438628Z","shell.execute_reply.started":"2023-04-12T00:45:44.438301Z","shell.execute_reply":"2023-04-12T00:45:44.438334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Precision and Recall**","metadata":{}},{"cell_type":"code","source":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.440101Z","iopub.status.idle":"2023-04-12T00:45:44.440710Z","shell.execute_reply.started":"2023-04-12T00:45:44.440391Z","shell.execute_reply":"2023-04-12T00:45:44.440422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = lrModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.442423Z","iopub.status.idle":"2023-04-12T00:45:44.443020Z","shell.execute_reply.started":"2023-04-12T00:45:44.442724Z","shell.execute_reply":"2023-04-12T00:45:44.442754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\nprint('Test Area Under ROC', evaluator.evaluate(predictions))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T00:45:44.444520Z","iopub.status.idle":"2023-04-12T00:45:44.445103Z","shell.execute_reply.started":"2023-04-12T00:45:44.444815Z","shell.execute_reply":"2023-04-12T00:45:44.444846Z"},"trusted":true},"execution_count":null,"outputs":[]}]}